{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddings.ipynb",
      "provenance": [],
      "mount_file_id": "1nYenWwwdRuFa-uW9Diief3T33PyvVsFG",
      "authorship_tag": "ABX9TyOynbuQJmN+P0JCCrndNCxi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kprhgJCEiqj1",
        "outputId": "f149e337-cc20-4d8e-cd3d-1b286ae7fa67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# import necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import conll2000\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z-Uts4Vis5D"
      },
      "source": [
        "# load POS tagged corpora from NLTK\n",
        "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
        "brown_corpus = brown.tagged_sents(tagset='universal')\n",
        "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
        "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z06zI3ZAlC-q"
      },
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    X_sentence = []\n",
        "    Y_sentence = []\n",
        "    for entity in sentence:         \n",
        "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "        \n",
        "    X.append(X_sentence)\n",
        "    Y.append(Y_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRBGlMRmoBjG"
      },
      "source": [
        "# encode X\n",
        "\n",
        "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence\n",
        "\n",
        "# encode Y\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Egk3dmoOM7",
        "outputId": "a617da5e-44ab-4339-c83f-84158d9d35c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# look at first encoded data point\n",
        "\n",
        "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X[0], '\\n')\n",
        "print('Y: ', Y[0], '\\n')\n",
        "print()\n",
        "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X_encoded[0], '\\n')\n",
        "print('Y: ', Y_encoded[0], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Raw data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
            "\n",
            "Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
            "\n",
            "\n",
            "** Encoded data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  [6423, 24231, 2, 7652, 102, 170, 2, 47, 1898, 1, 269, 17, 7, 13230, 619, 1711, 2761, 3] \n",
            "\n",
            "Y:  [1, 1, 3, 11, 1, 6, 3, 2, 2, 5, 1, 4, 5, 6, 1, 1, 11, 3] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWQUY1Ea89lJ",
        "outputId": "e7b30faf-88bf-4164-a817-006cea839715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sequences greater than 100 in length will be truncated\n",
        "MAX_SEQ_LENGTH = 100\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")# print the first sequence\n",
        "print(X_padded[0], \"\\n\"*3)\n",
        "print(Y_padded[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0  6423 24231\n",
            "     2  7652   102   170     2    47  1898     1   269    17     7 13230\n",
            "   619  1711  2761     3] \n",
            "\n",
            "\n",
            "\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  1  1  3 11  1  6  3  2  2  5  1  4  5  6\n",
            "  1  1 11  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBNUaHKJVcoW"
      },
      "source": [
        "X, Y = X_padded, Y_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw7vpoJiTo2a"
      },
      "source": [
        "path = '/content/drive/My Drive/Môn học/Kaggle/GoogleNews-vectors-negative300.bin'\n",
        "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDrN_7r4UDkr"
      },
      "source": [
        "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "# create an empty embedding matix\n",
        "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "# create a word to index dictionary mapping\n",
        "word2id = word_tokenizer.word_index\n",
        "\n",
        "# copy vectors from word2vec model to the words present in corpus\n",
        "for word, index in word2id.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass\n",
        "        \n",
        "Y = to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZktUgkDVp3K",
        "outputId": "f5f3c31b-34b5-4123-a559-859d115efb47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# split entire data into training and testing sets\n",
        "TEST_SIZE = 0.15\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
        "\n",
        "VALID_SIZE = 0.15\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)\n",
        "\n",
        "# print number of samples in each set\n",
        "print(\"TRAINING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_train.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"VALIDATION DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"TESTING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_test.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING DATA\n",
            "Shape of input sequences: (52165, 100)\n",
            "Shape of output sequences: (52165, 100, 13)\n",
            "--------------------------------------------------\n",
            "VALIDATION DATA\n",
            "Shape of input sequences: (9206, 100)\n",
            "Shape of output sequences: (9206, 100, 13)\n",
            "--------------------------------------------------\n",
            "TESTING DATA\n",
            "Shape of input sequences: (10831, 100)\n",
            "Shape of output sequences: (10831, 100, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCPHiUe3Xl-Q"
      },
      "source": [
        "NUM_CLASSES = Y.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNu-LcznXz9u"
      },
      "source": [
        "# create architecture\n",
        "\n",
        "rnn_model = Sequential()\n",
        "\n",
        "# create embedding layer - usually the first layer in text problems\n",
        "rnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n",
        "                        output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                        weights       = [embedding_weights],      # word embedding matrix\n",
        "                        trainable     =  True                     # True - update the embeddings while training\n",
        "))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "rnn_model.add(SimpleRNN(64, \n",
        "              return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
        "))\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n",
        "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_NFkGcYPxx"
      },
      "source": [
        "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe5tC2Cha9XT",
        "outputId": "5404b314-f6e7-4aa5-b101-0fb184d321c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check summary of the model\n",
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 300)          17834700  \n",
            "_________________________________________________________________\n",
            "simple_rnn_3 (SimpleRNN)     (None, 100, 64)           23360     \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 100, 13)           845       \n",
            "=================================================================\n",
            "Total params: 17,858,905\n",
            "Trainable params: 17,858,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIPqYSa5bAIY",
        "outputId": "5f0158ab-803e-4cda-a741-413d81960e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "408/408 [==============================] - 120s 294ms/step - loss: 0.1905 - acc: 0.9617 - val_loss: 0.0357 - val_acc: 0.9890\n",
            "Epoch 2/10\n",
            "408/408 [==============================] - 122s 299ms/step - loss: 0.0272 - acc: 0.9910 - val_loss: 0.0274 - val_acc: 0.9905\n",
            "Epoch 3/10\n",
            "408/408 [==============================] - 121s 296ms/step - loss: 0.0201 - acc: 0.9929 - val_loss: 0.0257 - val_acc: 0.9909\n",
            "Epoch 4/10\n",
            "408/408 [==============================] - 120s 294ms/step - loss: 0.0171 - acc: 0.9938 - val_loss: 0.0251 - val_acc: 0.9911\n",
            "Epoch 5/10\n",
            "408/408 [==============================] - 120s 294ms/step - loss: 0.0151 - acc: 0.9946 - val_loss: 0.0251 - val_acc: 0.9911\n",
            "Epoch 6/10\n",
            "408/408 [==============================] - 120s 294ms/step - loss: 0.0134 - acc: 0.9952 - val_loss: 0.0256 - val_acc: 0.9911\n",
            "Epoch 7/10\n",
            "408/408 [==============================] - 120s 295ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0263 - val_acc: 0.9910\n",
            "Epoch 8/10\n",
            "408/408 [==============================] - 120s 295ms/step - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0276 - val_acc: 0.9908\n",
            "Epoch 9/10\n",
            "408/408 [==============================] - 119s 293ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0291 - val_acc: 0.9907\n",
            "Epoch 10/10\n",
            "408/408 [==============================] - 119s 292ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0308 - val_acc: 0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}