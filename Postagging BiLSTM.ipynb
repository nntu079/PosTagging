{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nntu079/PosTagging/blob/main/Postagging%20BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kprhgJCEiqj1",
        "outputId": "eed8cec8-9c44-4a19-8b13-e6e0678e1ba8"
      },
      "source": [
        "# import necessary libraries\n",
        "import warnings\n",
        "import keras\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import conll2000\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z-Uts4Vis5D"
      },
      "source": [
        "# load POS tagged corpora from NLTK\n",
        "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
        "brown_corpus = brown.tagged_sents(tagset='universal')\n",
        "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
        "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z06zI3ZAlC-q",
        "outputId": "eee3016c-8792-4a8b-e9e0-e7a567e6eaad"
      },
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    X_sentence = []\n",
        "    Y_sentence = []\n",
        "    for entity in sentence:         \n",
        "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "        \n",
        "    X.append(X_sentence)\n",
        "    Y.append(Y_sentence)\n",
        "print(X[0])\n",
        "print(Y[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC5vajMy0eMO",
        "outputId": "cbe0d26a-872d-483a-ac42-aa40392737ea"
      },
      "source": [
        "X.append(['I','can','can','a','can','.'])\n",
        "Y.append(['PRON','VERB','VERB','DET','NOUN','.'])\n",
        "\n",
        "X.append(['I','can','can','a','can','.'])\n",
        "Y.append(['PRON','VERB','VERB','DET','NOUN','.'])\n",
        "\n",
        "X.append(['I','can','can','a','can','.'])\n",
        "Y.append(['PRON','VERB','VERB','DET','NOUN','.'])\n",
        "\n",
        "X.append(['I','can','can','a','can','.'])\n",
        "Y.append(['PRON','VERB','VERB','DET','NOUN','.'])\n",
        "\n",
        "X.append(['I','can','can','a','can','.'])\n",
        "Y.append(['PRON','VERB','VERB','DET','NOUN','.'])\n",
        "\n",
        "print( set((Y[-1]+Y[0]+Y[1]+Y[2]+Y[3]+Y[4]+Y[5]+Y[6]+Y[7]+Y[9]+Y[8]+Y[10]+Y[11]+Y[12]+Y[13]+Y[14]+Y[15]+Y[16])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'CONJ', 'X', 'ADJ', 'VERB', 'DET', 'ADP', 'ADV', 'NUM', 'NOUN', 'PRON', 'PRT', '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRBGlMRmoBjG",
        "outputId": "39468540-0f5b-4c9b-a754-da56625ca33d"
      },
      "source": [
        "# encode X\n",
        "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence\n",
        "\n",
        "with open('word_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(word_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# encode Y\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y)\n",
        "\n",
        "with open('tag_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tag_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(X_encoded[0])\n",
        "print(Y_encoded[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6423, 24231, 2, 7652, 102, 170, 2, 47, 1898, 1, 269, 17, 7, 13230, 619, 1711, 2761, 3]\n",
            "[1, 1, 3, 11, 1, 6, 3, 2, 2, 5, 1, 4, 5, 6, 1, 1, 11, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Egk3dmoOM7",
        "outputId": "33e121bb-3265-41b4-b8ba-f45ae62e2efe"
      },
      "source": [
        "# look at first encoded data point\n",
        "\n",
        "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X[0], '\\n')\n",
        "print('Y: ', Y[0], '\\n')\n",
        "print()\n",
        "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X_encoded[0], '\\n')\n",
        "print('Y: ', Y_encoded[0], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Raw data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
            "\n",
            "Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
            "\n",
            "\n",
            "** Encoded data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  [6423, 24231, 2, 7652, 102, 170, 2, 47, 1898, 1, 269, 17, 7, 13230, 619, 1711, 2761, 3] \n",
            "\n",
            "Y:  [1, 1, 3, 11, 1, 6, 3, 2, 2, 5, 1, 4, 5, 6, 1, 1, 11, 3] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWQUY1Ea89lJ",
        "outputId": "237c5e03-1e7d-43bf-d391-8d2c81051a2e"
      },
      "source": [
        "# sequences greater than 100 in length will be truncated\n",
        "MAX_SEQ_LENGTH = 100\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")# print the first sequence\n",
        "print(X_padded[0], \"\\n\"*3)\n",
        "print(Y_padded[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0  6423 24231\n",
            "     2  7652   102   170     2    47  1898     1   269    17     7 13230\n",
            "   619  1711  2761     3] \n",
            "\n",
            "\n",
            "\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  1  1  3 11  1  6  3  2  2  5  1  4  5  6\n",
            "  1  1 11  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBNUaHKJVcoW"
      },
      "source": [
        "X, Y = X_padded, Y_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HJHJou-Amtp",
        "outputId": "9016444e-359d-4143-f6fc-eaa0ed625901"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nw7vpoJiTo2a"
      },
      "source": [
        "path = '/content/drive/My Drive/Kaggle/GoogleNews-vectors-negative300.bin' #Tú chạy thì sửa lại đường dẫn phù hợp nha\n",
        "#đường dẫn của Tú: /content/drive/My Drive/Môn học/Kaggle/GoogleNews-vectors-negative300.bin\n",
        "path='/content/drive/My Drive/Môn học/Kaggle/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qDrN_7r4UDkr"
      },
      "source": [
        "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "# create an empty embedding matix\n",
        "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "# create a word to index dictionary mapping\n",
        "word2id = word_tokenizer.word_index\n",
        "\n",
        "# copy vectors from word2vec model to the words present in corpus\n",
        "for word, index in word2id.items():    \n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass\n",
        "        \n",
        "Y = to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZktUgkDVp3K",
        "outputId": "3a2fdea8-48ea-459d-9974-b8429ae4421d"
      },
      "source": [
        "# split entire data into training and testing sets\n",
        "TEST_SIZE = 0.05\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
        "\n",
        "VALID_SIZE = 0.25\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)\n",
        "\n",
        "# print number of samples in each set\n",
        "print(\"TRAINING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_train.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"VALIDATION DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"TESTING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_test.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING DATA\n",
            "Shape of input sequences: (51522, 100)\n",
            "Shape of output sequences: (51522, 100, 14)\n",
            "--------------------------------------------------\n",
            "VALIDATION DATA\n",
            "Shape of input sequences: (17174, 100)\n",
            "Shape of output sequences: (17174, 100, 14)\n",
            "--------------------------------------------------\n",
            "TESTING DATA\n",
            "Shape of input sequences: (3616, 100)\n",
            "Shape of output sequences: (3616, 100, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PCPHiUe3Xl-Q"
      },
      "source": [
        "NUM_CLASSES = Y.shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nNu-LcznXz9u"
      },
      "source": [
        "# create architecture\n",
        "\n",
        "rnn_model = Sequential()\n",
        "\n",
        "# create embedding layer - usually the first layer in text problems\n",
        "rnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n",
        "                        output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                        weights       = [embedding_weights],      # word embedding matrix\n",
        "                        trainable     =  True                     # True - update the embeddings while training\n",
        "))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "rnn_model.add(Bidirectional(SimpleRNN(64, \n",
        "              return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
        ")))\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n",
        "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hi60igj76hdM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vr_NFkGcYPxx"
      },
      "source": [
        "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe5tC2Cha9XT",
        "outputId": "713bbe8a-44b8-48c5-cd7f-5e9407a0fc4c"
      },
      "source": [
        "# check summary of the model\n",
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 300)          17834700  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 100, 128)          46720     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 100, 14)           1806      \n",
            "=================================================================\n",
            "Total params: 17,883,226\n",
            "Trainable params: 17,883,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIPqYSa5bAIY",
        "outputId": "1ba112d7-b433-4311-bb33-3da43692da88"
      },
      "source": [
        "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "403/403 [==============================] - 163s 404ms/step - loss: 0.2191 - acc: 0.9503 - val_loss: 0.0480 - val_acc: 0.9877\n",
            "Epoch 2/10\n",
            "403/403 [==============================] - 162s 403ms/step - loss: 0.0305 - acc: 0.9914 - val_loss: 0.0279 - val_acc: 0.9911\n",
            "Epoch 3/10\n",
            "403/403 [==============================] - 155s 386ms/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0242 - val_acc: 0.9920\n",
            "Epoch 4/10\n",
            "403/403 [==============================] - 158s 392ms/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.0228 - val_acc: 0.9924\n",
            "Epoch 5/10\n",
            "403/403 [==============================] - 158s 391ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0222 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "403/403 [==============================] - 160s 397ms/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0222 - val_acc: 0.9926\n",
            "Epoch 7/10\n",
            "403/403 [==============================] - 159s 395ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0227 - val_acc: 0.9926\n",
            "Epoch 8/10\n",
            "403/403 [==============================] - 158s 392ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0235 - val_acc: 0.9925\n",
            "Epoch 9/10\n",
            "403/403 [==============================] - 159s 394ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0245 - val_acc: 0.9925\n",
            "Epoch 10/10\n",
            "403/403 [==============================] - 158s 392ms/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0257 - val_acc: 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5bEeVXgDywhT"
      },
      "source": [
        "rnn_model.save('/content/drive/MyDrive/Môn học/NLP/Model/model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb3Kh4CR7_h0"
      },
      "source": [
        "#Because practice makes perfect, we suggest you start learning how to wrap a gift right now\n",
        "sample=input()\n",
        "X_sample=sample.split()\n",
        "X_sample\n",
        "X_temp=[]\n",
        "X_temp.append(X_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0bhyV-eYNrMt",
        "outputId": "bb532dd4-147b-4c1c-cc26-e4e31a331b04"
      },
      "source": [
        "X_temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', 'can', 'can', 'a', 'can']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zc22tWB68Hlm",
        "outputId": "fb84fcae-7727-4614-8435-81a49f085d21"
      },
      "source": [
        "# encode X\n",
        "X_sample_encoded = word_tokenizer.texts_to_sequences(X_temp)  # use the tokeniser to encode input sequence\n",
        "print(X_sample_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[29, 69, 69, 7, 69]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT6voffLD4FR"
      },
      "source": [
        "# sequences greater than 100 in length will be truncated\n",
        "X_sample_padded = pad_sequences(X_sample_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "print(X_sample_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sug9XH4MJyqX"
      },
      "source": [
        "t=rnn_model.predict_classes(X_sample_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2zqhW6-LylH"
      },
      "source": [
        "print(sample)\n",
        "print(tag_tokenizer.sequences_to_texts(t)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}